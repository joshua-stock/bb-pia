{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:17:22.213591Z",
     "start_time": "2024-06-14T10:17:19.633703Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cifar_functions import data_train_test_cifar\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, sensitive, sensitive_t = data_train_test_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "7149e9163d9576f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:17:22.222764Z",
     "start_time": "2024-06-14T10:17:22.216592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "6aa6a24766853214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:44:06.128216Z",
     "start_time": "2024-06-14T11:44:06.045483Z"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import RandomFlip, Conv2D, GroupNormalization, MaxPooling2D, Dense, Flatten\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras import Sequential, Input\n",
    "from keras.src.saving.saving_api import load_model, save_model\n",
    "from keras.utils import set_random_seed\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "class DatasetWithForcedDistribution:\n",
    "    def __init__(self, sensitive_attribute_name, distribution, X_train, X_test, y_train, y_test, sensitive, sensitive_t):\n",
    "        self.sensitive_attribute_name = sensitive_attribute_name\n",
    "        self.distribution = distribution\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.sensitive = sensitive\n",
    "        self.sensitive_t = sensitive_t\n",
    "\n",
    "def ensure_path_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def drop_index(df, idx):\n",
    "    return df.reset_index().drop(index=idx).drop(columns=[\"index\"])\n",
    "\n",
    "def find_indices_to_drop(sensitive, target_distribution):\n",
    "    length = len(sensitive)\n",
    "    indices_to_drop = []\n",
    "    def current_dist(sensitive_value=1):\n",
    "        if sensitive_value == 1:\n",
    "            return (pd.Series(sensitive).value_counts()[1] - len(indices_to_drop)) / (length - len(indices_to_drop))\n",
    "        else:\n",
    "            return (pd.Series(sensitive).value_counts()[1]) / (length - len(indices_to_drop))\n",
    "\n",
    "    if current_dist() > target_distribution:\n",
    "        comp = operator.gt\n",
    "        sensitive_value_to_delete = 1\n",
    "    else:\n",
    "        comp = operator.lt\n",
    "        sensitive_value_to_delete = 0\n",
    "\n",
    "    i = 0\n",
    "    while comp(current_dist(sensitive_value_to_delete), target_distribution):\n",
    "        if i >= length:\n",
    "            raise ValueError(\"Unable to reach target distribution. Not enough entries with the sensitive value to delete.\")\n",
    "        if sensitive[i] == sensitive_value_to_delete:\n",
    "            diff = abs(current_dist(sensitive_value_to_delete) - target_distribution)\n",
    "            if diff > 0.1:\n",
    "                indices_to_drop.extend([i + j for j in range(10) if i + j < length])\n",
    "                i += 10\n",
    "            else:\n",
    "                indices_to_drop.append(i)\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return indices_to_drop\n",
    "\n",
    "def get_lucasnet_sequence(num_classes, input_shape):\n",
    "    groups = 32\n",
    "    return [\n",
    "        Input(shape=input_shape),\n",
    "        RandomFlip(\"horizontal\", seed=42),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=\"same\", activation=\"relu\"),\n",
    "        GroupNormalization(groups=groups),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=\"same\", activation=\"relu\"),\n",
    "        GroupNormalization(groups=groups),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", activation=\"relu\"),\n",
    "        GroupNormalization(groups=groups),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        GroupNormalization(groups=groups),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    "\n",
    "def get_lucasnet_model(num_classes, input_shape):\n",
    "    return Sequential(get_lucasnet_sequence(num_classes, input_shape))\n",
    "\n",
    "def compile_lucasnet(model):\n",
    "    model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fit_lucasnet(X_train, y_train, X_test, y_test, batch_size=32, epochs=5, verbose=0, input_shape=(64, 64, 3), num_classes=2):\n",
    "    model = get_lucasnet_model(num_classes, input_shape)\n",
    "    model = compile_lucasnet(model)\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=None if X_test is None else (X_test, y_test), verbose=verbose)\n",
    "    return model, history\n",
    "\n",
    "def train_and_generate_output(X_train, y_train, shadow_input, load_model_path, save_model_path, model_no, input_shape, num_classes):\n",
    "    if os.path.isfile(f\"{load_model_path}{model_no}.keras\"):\n",
    "        print(f\"Loading model {model_no}\")\n",
    "        shadow_model = load_model(f\"{load_model_path}{model_no}.keras\")\n",
    "    else:\n",
    "        set_random_seed(model_no)\n",
    "        shadow_model, _ = fit_lucasnet(X_train, y_train, X_test=None, y_test=None, input_shape=input_shape, num_classes=num_classes)\n",
    "        if save_model_path is not None:\n",
    "            shadow_model.save(f\"{save_model_path}{model_no}.keras\")\n",
    "    output = np.array(shadow_model.predict(shadow_input, verbose=0)).astype(np.float16)\n",
    "    return output[:, 0:output.shape[1]-1]\n",
    "\n",
    "def generate_shadow_model_outputs(dataset, shadow_input, load_model_path, save_model_path, n_shadow_models=100, use_test_data=False, input_shape=(64, 64, 3), num_classes=2):\n",
    "    if use_test_data:\n",
    "        X = dataset.X_test\n",
    "        y = dataset.y_test\n",
    "    else:\n",
    "        X = dataset.X_train\n",
    "        y = dataset.y_train\n",
    "\n",
    "    outputs = [train_and_generate_output(X, y, shadow_input, load_model_path, save_model_path, i, input_shape, num_classes) for i in range(n_shadow_models)]\n",
    "    outputs = np.array([o.flatten() for o in outputs])\n",
    "    return outputs\n",
    "\n",
    "def train_shadow_models(test_run, n_shadow_models, distributed_datasets, model_input, input_shape, num_classes, base_path, save_models=True):\n",
    "    for ds in distributed_datasets:\n",
    "        print(f\"now generating {ds.distribution}...\")\n",
    "        load_model_path = f\"{base_path}/models/shadow_models/{str(ds.distribution)}/{'test' if test_run else 'train'}/\"\n",
    "        if save_models:\n",
    "            save_model_path = f\"{base_path}/models/shadow_models/{str(ds.distribution)}/{'test' if test_run else 'train'}/\"\n",
    "            ensure_path_exists(save_model_path)\n",
    "        else:\n",
    "            save_model_path = None\n",
    "        outputs = generate_shadow_model_outputs(ds, model_input, load_model_path, save_model_path, n_shadow_models=n_shadow_models, use_test_data=test_run, input_shape=input_shape, num_classes=num_classes)\n",
    "        adv_df = pd.DataFrame(outputs)\n",
    "        adv_df[\"y\"] = np.repeat(ds.distribution, n_shadow_models)\n",
    "        save_data_path = f\"{base_path}/data/shadow_model_outputs/{str(ds.distribution)}/\"\n",
    "        ensure_path_exists(save_data_path)\n",
    "        adv_df.to_csv(f\"{save_data_path}{'test' if test_run else 'train'}.csv\", index=False)\n",
    "\n",
    "class DefendingModel(keras.Sequential):\n",
    "    def __init__(self, pia_adversary, adv_input, sensitive, training_lambda, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pia_adversary = pia_adversary\n",
    "        self.adv_input = adv_input\n",
    "        self.sensitive = sensitive\n",
    "        self.training_lambda = training_lambda\n",
    "        self.adversary_metric = keras.metrics.Mean(name='adversary_prediction')\n",
    "        self.p_rule_metric = keras.metrics.Mean(name='p_rule')\n",
    "\n",
    "        # Initialize TensorFlow variables for the fairness adversary\n",
    "        self.init_fairness_adversary()\n",
    "    \n",
    "    @staticmethod\n",
    "    def p_rule(y_pred, z_values, threshold=0.5):\n",
    "        y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "        y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "        odds = y_z_1.mean() / y_z_0.mean()\n",
    "        return np.min([odds, 1/odds])\n",
    "\n",
    "    def init_fairness_adversary(self):\n",
    "        adv = keras.Sequential([\n",
    "            Input(shape=(10,)),\n",
    "            Dense(5, activation='relu'),\n",
    "            Dense(2, activation='softmax')\n",
    "        ])\n",
    "        adv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        self.adversary = adv\n",
    "\n",
    "    def train_fairness_adversary(self, y_pred_for_adv):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            adv_pred = self.adversary(y_pred_for_adv)\n",
    "            # Compute the loss value\n",
    "            adv_loss = tf.keras.losses.BinaryCrossentropy()(self.sensitive, adv_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.adversary.trainable_variables\n",
    "        gradients = tape.gradient(adv_loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.adversary.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "    #def train_pia_adversary(\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        y_pred_for_adv = self(x, training=False)\n",
    "\n",
    "        # Train the fairness adversary\n",
    "        self.train_fairness_adversary(y_pred_for_adv)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred_for_adv = self(x, training=False)\n",
    "            #y_pred_for_adv = tf.expand_dims(tf.cast(y_pred_for_adv, dtype=tf.float32), axis=1)\n",
    "            \n",
    "            adv_pred = self.adversary(y_pred_for_adv, training=False)\n",
    "            adv_loss = tf.keras.losses.BinaryCrossentropy()(self.sensitive, adv_pred)\n",
    "\n",
    "            y_pred = self(x, training=True)\n",
    "            loss_train = self.compute_loss(x, y, y_pred)\n",
    "            combined_loss = loss_train * (1-self.training_lambda) + self.training_lambda * adv_loss\n",
    "\n",
    "        gradients = tape.gradient(combined_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        metrics = self.compute_metrics(x, y, y_pred, sample_weight=None)\n",
    "        \n",
    "        # check pia adversary output\n",
    "        y_pred_for_adv = self(self.adv_input, training=False)\n",
    "        # last column of prediction is redundant\n",
    "        num_columns = y_pred_for_adv.shape[1]-1\n",
    "        y_pred_for_adv = y_pred_for_adv[:, 0:num_columns]\n",
    "        y_pred_for_adv = Flatten()(y_pred_for_adv)\n",
    "        # reshape as model input\n",
    "        my_x = tf.reshape(y_pred_for_adv, (1, y_pred_for_adv.shape[0]*y_pred_for_adv.shape[1]))\n",
    "        \n",
    "        metrics.update({'adversary_prediction': self.pia_adversary(my_x),\n",
    "                        'p_rule': self.p_rule(self(x, training=False), self.sensitive)})\n",
    "        return metrics\n",
    "\n",
    "    def save_inner_model(self, filepath):\n",
    "        seq = keras.Sequential(self.layers)\n",
    "        seq = compile_categorical_model(seq)\n",
    "        save_model(seq, filepath)\n",
    "\n",
    "\n",
    "def get_fairness_lucasnet_model(pia_adversary, adversary_input, sensitive, training_lambda, num_classes, input_shape):\n",
    "    return DefendingModel(pia_adversary, adversary_input, sensitive, training_lambda, get_lucasnet_sequence(num_classes, input_shape))\n",
    "\n",
    "def compile_categorical_model(model):\n",
    "    model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "4c2122c2be6f22b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:44:09.242801Z",
     "start_time": "2024-06-14T11:44:06.245608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_613361/3455634472.py\", line 217, in train_step  *\n        metrics.update({'adversary_prediction': self.pia_adversary(my_x),\n    File \"/tmp/ipykernel_613361/1057371444.py\", line 153, in p_rule  *\n        y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n\n    ValueError: Shapes (30000, 10) and (30000, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[379], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m get_fairness_lucasnet_model(\n\u001b[1;32m      3\u001b[0m     pia_adv,\n\u001b[1;32m      4\u001b[0m     input_set,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m compile_categorical_model(model)\n\u001b[0;32m---> 11\u001b[0m model_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/informatik2/svs/home/stock/anaconda3/envs/pia-tf2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5fsh8xnq.py:28\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     26\u001b[0m y_pred_for_adv \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(Flatten), (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), (ag__\u001b[38;5;241m.\u001b[39mld(y_pred_for_adv),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     27\u001b[0m my_x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred_for_adv), (\u001b[38;5;241m1\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(y_pred_for_adv)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_pred_for_adv)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 28\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(metrics)\u001b[38;5;241m.\u001b[39mupdate, ({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madversary_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpia_adversary, (ag__\u001b[38;5;241m.\u001b[39mld(my_x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_rule\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensitive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m},), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileu3a89hrx.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__p_rule\u001b[0;34m(y_pred, z_values, threshold)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m y_z_1 \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m y_z_0 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp(ag__\u001b[38;5;241m.\u001b[39mld(threshold), \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(y_pred)[ag__\u001b[38;5;241m.\u001b[39mld(z_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(threshold), \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(y_pred)[ag__\u001b[38;5;241m.\u001b[39mld(z_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m odds \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(y_z_1)\u001b[38;5;241m.\u001b[39mmean, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(y_z_0)\u001b[38;5;241m.\u001b[39mmean, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileu3a89hrx.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__p_rule.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m y_z_1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp(ag__\u001b[38;5;241m.\u001b[39mld(threshold), \u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(threshold), \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(y_pred)[ag__\u001b[38;5;241m.\u001b[39mld(z_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m y_z_0 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp(ag__\u001b[38;5;241m.\u001b[39mld(threshold), \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(y_pred)[ag__\u001b[38;5;241m.\u001b[39mld(z_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(threshold), \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(y_pred)[ag__\u001b[38;5;241m.\u001b[39mld(z_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m odds \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(y_z_1)\u001b[38;5;241m.\u001b[39mmean, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(y_z_0)\u001b[38;5;241m.\u001b[39mmean, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_613361/3455634472.py\", line 217, in train_step  *\n        metrics.update({'adversary_prediction': self.pia_adversary(my_x),\n    File \"/tmp/ipykernel_613361/1057371444.py\", line 153, in p_rule  *\n        y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n\n    ValueError: Shapes (30000, 10) and (30000, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "sensitive_categorical = tf.keras.utils.to_categorical(sensitive)\n",
    "model = get_fairness_lucasnet_model(\n",
    "    pia_adv,\n",
    "    input_set,\n",
    "    sensitive=sensitive_categorical,\n",
    "    training_lambda=0.0,\n",
    "    num_classes=10,\n",
    "    input_shape=(32, 32, 3))\n",
    "model = compile_categorical_model(model)\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    batch_size=30000,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)]\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "8ce228196e2b9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar adv\n",
    "def create_and_compile(input_shape):\n",
    "    manual_adversary = keras.Sequential()\n",
    "    manual_adversary.add(keras.Input(shape=input_shape))\n",
    "    manual_adversary.add(keras.layers.Dense(30, activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "    manual_adversary.add(keras.layers.Dropout(0.05))\n",
    "    manual_adversary.add(keras.layers.Dense(8, activation='relu'))#, kernel_regularizer=regularizers.l2(0.04)))\n",
    "    manual_adversary.add(keras.layers.Dense(1))\n",
    "    manual_adversary.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.R2Score()])\n",
    "    return manual_adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "8aee47f8dc8c1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pia_adv = create_and_compile((45360,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "ac578cce4a7caa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pia_adv.load_weights(\"cifar/models/cifar_adv_v2_0.59_test_r2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5a2f76e4b6c83d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar_functions import get_cifar_input_set\n",
    "\n",
    "input_set = get_cifar_input_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "6e65af367dae55a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:31:31.707967Z",
     "start_time": "2024-06-14T11:31:31.695383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.array(sensitive).value_counts().iloc[0] / len(sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb39818a96fee55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:28:01.225832Z",
     "start_time": "2024-06-14T10:28:01.223929Z"
    }
   },
   "outputs": [],
   "source": [
    "model_l0 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "fb340b98795baed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:14:22.591368Z",
     "start_time": "2024-05-31T12:14:22.584897Z"
    }
   },
   "outputs": [],
   "source": [
    "model_l0 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "23e03722d4ac1e6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:37:21.612673Z",
     "start_time": "2024-05-31T12:37:17.386277Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_0 = model_l0(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "4dc4dc04e0c0c428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:37:43.819702Z",
     "start_time": "2024-05-31T12:37:39.636105Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_01 = model_l01(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "611ad574a8909490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:18:10.376626Z",
     "start_time": "2024-05-31T11:18:10.367846Z"
    }
   },
   "outputs": [],
   "source": [
    "adv_pred = model.adversary(np.reshape(pred, (30000, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ee6033acb069d3aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:18:10.384474Z",
     "start_time": "2024-05-31T11:18:10.379177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30000, 2), dtype=float32, numpy=\n",
       "array([[0.50858974, 0.4914103 ],\n",
       "       [0.5120358 , 0.4879643 ],\n",
       "       [0.5096619 , 0.4903381 ],\n",
       "       ...,\n",
       "       [0.51186997, 0.48812997],\n",
       "       [0.51150024, 0.48849967],\n",
       "       [0.50139654, 0.4986034 ]], dtype=float32)>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b66e0eec9b515423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:09:58.885997Z",
     "start_time": "2024-05-31T11:09:58.876258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, ..., False,  True,  True])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "989a966e5836333e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:11:00.839114Z",
     "start_time": "2024-05-31T11:11:00.827691Z"
    }
   },
   "outputs": [],
   "source": [
    "def p_rule(y_pred, z_values, threshold=0.5):\n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = y_z_1.mean() / y_z_0.mean()\n",
    "    return np.min([odds, 1/odds]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d913e38341ffa037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:43:59.324301Z",
     "start_time": "2024-06-14T11:43:59.309825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.655925528105975"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_rule(pred_0.numpy(), sensitive) # lambda = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "300658cd1f68a852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:45:32.153351Z",
     "start_time": "2024-06-14T11:45:32.142245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.73178142383023"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_rule(pred_01.numpy(), sensitive) # lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "15fdd50d740e2ac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:42:44.636808Z",
     "start_time": "2024-06-14T11:42:44.625941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([30000, 10])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "60eebdfe16208419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:42:50.823952Z",
     "start_time": "2024-06-14T11:42:50.814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "a8e0a7a918544448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:46:28.581157Z",
     "start_time": "2024-06-14T11:46:28.559948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18000, 10), dtype=bool, numpy=\n",
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])>"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_01[sensitive == 1] > 0.5# if threshold else y_pred[sensitive == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "949157c5fb0fffbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:47:54.262551Z",
     "start_time": "2024-06-14T11:47:54.250894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "9\n",
      "3\n",
      "1\n",
      "2\n",
      "9\n",
      "0\n",
      "1\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for p in pred_01[:10]:\n",
    "    # return index of element with highest value in p:\n",
    "    print(np.argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947403c64a083efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pia-tf2",
   "language": "python",
   "name": "pia-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
